# Superset Chart Generator for ClickHouse (CBR Data)
## Описание
Это приложение, используя API, автоматически загружает статистические данные с сайта [ЦБ РФ](https://www.cbr.ru/), сохраняет их в базе данных ClickHouse и позволяет создавать графики в Apache Superset, используя один из нескольких автоматизированных CLI-сценариев, через его REST API. Актуальность данных обеспечивается при помощи Apache Airflow, который позволяет в автоматическом режиме их обновлять.  

## Возможности

- Интерактивный выбор публикации, показателя и разреза
- Анализ данных по выбранному периоду
- Автоматическое создание датасета и графика по выбранному сценарию

## Концептуальная схема приложения
![Project (5)](https://github.com/user-attachments/assets/76c0da8e-090c-44c7-b64d-4645fb28eb6d)

## Используемая структура таблиц базы данных для хранения
![Project (4)](https://github.com/user-attachments/assets/d7dedcd2-d495-4f96-bcc9-87cede762fb7)
Основной является таблица **data**, которая в дальнейшем используется для визуализации в superset и которая обновляется с помощью Apache Airflow.

## Структура проекта
![image](https://github.com/user-attachments/assets/67374019-76a4-45bb-aa41-db806a87f646)

<details> 
  <summary>Компонент airflow</summary>
  Данный компонент содержит в себе Dockerfile с описанием параметров создания контейнера в Docker, а также директорию dags, в которой находится скрипт update_datasets.py, использующий Apache Airflow для периодизации обновления данных. 

  Использующиеся функции и методы:

  1. **sql_value(value)** - используется для преобразования None-значения в NULL для корректной вставки в БД, а также оборачивания строкового значения в кавычки.
  2. **def put_data(client, publication_id, dataset_id, measure_id, from_date, to_date)** - использует подключение к БД, идентификатор публикации, показателя и разреза для поиска и вставки в БД значений, соответствующих периоду между датами from_date и to_date
  3. **def update_datasets()** - скрипт для DAG-задания, который будет выполняться в соответствии с заранее заданным интервалом (в данном случае - ежедневно). Устанавливает подключение к БД, проверяет, какие данные уже устарели, вытаскивает новые данные из API и с помощью функции **put_data()** помещает их в БД, обновляя сроки последнего изменения в таблице показателей. 

Структура DAG update_datasets, отвечающего за автоматическое обновление данных в ClickHouse с помощью Apache Airflow:
![Project (6)](https://github.com/user-attachments/assets/3abf8f79-a836-49b8-beb7-ef1aa37b3ccd)

</details>

<details> 
  <summary>Компонент loader</summary>

  Данный компонент содержит в себе Dockerfile с описанием параметров создания контейнера в Docker, конфигурационные файлы для построения контейнера **requirements.txt** и **entrypoint.sh**, а также два скрипта: **create_schema.py** - для создания БД и её структурных элементов - таблиц, **load_data.py** - для загрузки всех данных с сайта ЦБ РФ. 
  Используемые функции и методы в **create_schema.py**:
  1. **get_client()** - для установления соединения с БД для возможности дальнейшего взаимодействия с ней (загрузки, модификации и обновления данных).
  2. **command()** - для выполнения SQL-запроса, в данном случае - CREATE DATABASE ... и CREATE TABLE ... 

  Используемые функции и методы в **load_data.py**:
  1. **is_already_loaded(publication_id, dataset_id, measure_id)** - позволяет обеспечить проверку по идентификатору публикации, показателя и разреза, были ли загружены ранее начальные данные. Позволяет избежать дублирования строк для обеспечения корректности загрузки  
  2. **mark_as_loaded(publication_id, dataset_id, measure_id)** - помечает данные, связанные с идентификатором публикации, показателя и разреза, как уже загруженные. Работает в связке с предыдущей функцией.
  3. **get_data_batch(publication_id, dataset_id, measure_id)** - по заданным идентификаторам публикации, показателя и разреза возвращает соответствующие им данные. 
  4. **insert_batch(insert_query_part, rows)** - используя заранее подготовленный SQL-запрос и массив строк, которые необходимо загрузить в БД, позволяет выполнить запрос INSERT со вставкой в БД.  
  5. **fill_publications()** - заполняет таблицу publications базы данных, совершая перебор по всем публикациям, полученным через API.
  6. **fill_datasets()** - аналогично предыдущей функции заполняет таблицу datasets.
  7. **fill_datasets()** - аналогично предыдущей функции заполняет таблицу measures.
  8. **fill_year_ranges()** - аналогично предыдущей функции заполняет таблицу year_ranges.
  9. **fill_units()** - аналогично предыдущей функции заполняет таблицу units.
  10. **fill_data()** - аналогично предыдущей функции заполняет таблицу data.
  11. **fill_tables()** - вызывает все функции, заполняющие таблицы.
</details>

<details> 
  <summary>Компонент superset</summary>

  Данный компонент содержит в себе Dockerfile с описанием параметров создания контейнера в Docker, а также конфигурационный файл **superset_config.py**, содержащий дополнительные параметры подключения.
</details>

<details> 
  <summary>Компонент utils</summary>

  Содержит в себе модуль **request_utils.py**, содержащий вспомогательные функции для работы с HTTP-запросами:
  1. **log_each_retry(retry_state)** - используется для логирования попытки восстановления соединения, если оно было прервано в ходе выполнения HTTP-запроса.
  2. **fetch(endpoint, params=None)** - используется для выполнения GET-запроса по адресу `http://www.cbr.ru/dataservice/{endpoint}`. Функция обёрнута в декоратор `@retry` для возможности повторной отправки запроса при возникновении какой-либо ошибки во время его первичной отправки.
</details>

<details> 
  <summary>Скрипт MainScript.py</summary>

  Данный скрипт предоставляет интерфейс, позволяющий взаимодействовать пользователю с приложением, интерактивно выбирая опции для фильтрации данных, сценария построения и типа графика для последующей визулизации отобранных данных. Содержит в себе следующие функции и методы:
  1. **get_superset_tokens()** - метод, который получает токены доступа и безопасности в авторизованной сессии пользователя, используемые для подключения к API superset с последующей возможностью создания и модификации dataset'ов и chart'ов.
  2. **create_clickhouse_connection(session, csrf_token)** - создаёт подключение к БД clickhouse, используя ранее авторизованную сессию и csrf-токен безопасности.
  3. **create_dataset(session, csrf_token, database_name, schema_name, table_name)** - позволяет создать выборку данных, используя авторизованную сессию, csrf-токен безопасности, имя БД и таблицы.
  4. **get_datasource_id(table_name, schema, session)** - вспомогательная функция, позволяющая узнать идентификатор таблицы в подключенной базе данных.
  5. **select_publication()** - функция, предлагающая пользователю интерактивный выбор необходимой ему публикации.
  6. **select_dataset(publication_id)** - функция, предлагающая пользователю интерактивный выбор необходимого ему показателя.
  7. **select_measure(dataset_id, dataset_type)** - функция, предлагающая пользователю интерактивный выбор необходимого ему разреза.
  8. **select_years(dataset_id, measure_id)** - функция, предлагающая пользователю интерактивный выбор необходимого ему периода.
  9. **select_chart_type()** - функция, предлагающая пользователю интерактивный выбор необходимого ему типа графика.
  10. **select_payload_type(dataset)** - функция, предлагающая пользователю интерактивный выбор необходимого ему сценария построения графика.
  11. **run(publication, dataset, measure, from_year, to_year, viz_type, metrics)** - функция, исполняющая скрипт на основе выбранных пользователем опций. 
</details>

<details> 
  <summary>Конфигурационный docker-compose.yml файл</summary>
  Содержит в себе описание конфигурации и связей между контейнерами Docker приложения
</details>

<details> 
  <summary>Документационный README.md файл</summary>
  Содержит в себе описание приложения в формате Markdown, инструкции по его установке и запуску, а также необходимому программному обеспечению.
</details>

## Системные и программные требования
Минимально рекомендуемые характеристики:
- Операционная система: Windows 10 / Ubuntu 20.04+ (64-bit)
- Процессор: 2-ядерный (например, Intel i3 или AMD Ryzen 3)
- Оперативная память: от 4 ГБ
- Свободное место на диске: 1–2 ГБ
- Docker: >= 20.10.0
- Python: >= 3.8

## Инструкции по установке и настройке окружения для работы с приложением

1. Убедитесь, что на вашей машине установлены Docker и Python соответствующей версии, а также необходимые зависимости (см. пункт **Системные и программные требования**)
2. Скачайте файлы в виде ZIP-архива и распакуйте в желаемой директории либо клонируйте репозиторий:

   ```bash
   git clone https://github.com/NickFymann/CenterInvestPractice.git
   cd CenterInvestPractice
3. Разверните окружение:
   ```bash
   docker-compose up --build
4. Дождитесь заполнения базы данных
5. Сервисы Apache Superset и Apache Airflow доступны по адресам http://localhost:8088/login/ и http://localhost:8080/login/
6. Для запуска скрипта MainScript.py установите дополнительные библиотеки
   ```bash
   pip install requests==2.31.0 pandas==2.2.2 clickhouse-connect==0.6.7
   ```
   После авторизации в Apache Superset (по умолчанию login: admin, password: admin), вы сможете перейти ссылке, полученной в ходе выполнения скрипта, на чарт, где будет построен желаемый график.
7. Для корректной работы Apache Airflow установите дополнительные библиотеки
   ```bash
   pip install apache-airflow==2.9.0 tenacity==8.2.3 python-dateutil==2.9.0.post0
   ```
   После авторизации в Apache Airflow  (по умолчанию login: admin, password: admin), необходимо будет активировать DAG (update_datasets), переключив ползунок возле его имени, чтобы запустить периодическое выполнение обновления данных (по умолчанию ползунок находится в положении выкл.)![image](https://github.com/user-attachments/assets/bb66057b-61bd-43ed-80de-1547abf78642)
8. Приложение готов к использованию!

## Как использовать приложение?

1. Убедитесь, что выполнили все инструкции, описанные в разделе **Инструкции по установке и настройке окружения для работы с приложением**
2. Запустите скрипт MainScript. Это можно сделать при помощи средств различных сред разработок по типу Visual Studio Code, PyCharm и т.д., либо же при помощи командной строки Windows/терминала Linux командой 
   ```bash
   python MainScript.py
   ```
   Предварительно убедитесь, что находитесь в директории, где располагается **MainScript.py**. Cделать это можно с помощью команды
   ```bash
   pwd
   ```
   для Linux и
   ```bash
   cd
   ```
   для Windows.
3. В консоли введите идентификатор желаемой публикации из списка доступных:
   ![image](https://github.com/user-attachments/assets/cc240f01-c0e1-4574-8886-efd49c8bb3b0)
4. В консоли введите идентификатор желаемого показателя из списка доступных:
   ![image](https://github.com/user-attachments/assets/30c9f36c-f712-4d2c-9699-c150354e904c)

5. В консоли введите идентификатор желаемого разреза из списка доступных:
   ![image](https://github.com/user-attachments/assets/d295bc38-2d1a-4282-a73c-5217a36782b3)


6. В консоли введите начало и конец желаемого периода в соответствии с доступными годами:
   ![image](https://github.com/user-attachments/assets/354f0b92-4e31-407c-924c-67d4dcb54673)

7. В консоли введите номер желаемого графика, который будет использован при построении, из списка доступных:
   ![image](https://github.com/user-attachments/assets/2285a6df-842e-44ad-bb5c-0bfb4b64b8f3)

8. В консоли введите номер желаемого сценария, который будет использован при построении. На данный момент доступны два сценария: изменения среднемесячного значения показателя и помесячное изменение минимума и максимума значения показателя. 
   ![image](https://github.com/user-attachments/assets/c5b24dd0-f398-45da-a17b-6bb190ec068a)

9. В консоли появится ссылка на чарт, который был построен в соответствии с параметрами, которые вы выбрали на предыдущих шагах. 
   ![image](https://github.com/user-attachments/assets/e61a114f-9fff-45ad-a64a-3655210cb255)
   Перейдя по этой ссылке вы попадёте на страницу Apache Superset (возможно, вам предложат повторно авторизоваться. По умолчанию login: admin, password: admin), где будет изображение построенного графика.
   ![echarts-timeseries-line-в-целом-по-российской-федерации-средневзвешенные-процентные-ставки-по-кредитам-предоставленным-кредитными-организациями-нефинансовым-организациям-в-рублях-2025-05-16T15-20-21 576Z](https://github.com/user-attachments/assets/9aacf969-dce4-4f47-baa6-c2d4a7d7787f)
